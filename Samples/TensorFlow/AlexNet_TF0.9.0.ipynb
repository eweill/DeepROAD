{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking VDC Dataset with LMDB and TF 0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "import sys\n",
    "sys.path.append('/opt/caffe/python')\n",
    "\n",
    "from cStringIO import StringIO\n",
    "from caffe.proto import caffe_pb2\n",
    "from datetime import datetime\n",
    "import lmdb, logging, math, os, time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmdb_dir = '/data/VDC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBEntries(object):\n",
    "    '''Handle lmdb entries.\n",
    "    \n",
    "    Attributes:\n",
    "        begin: Int; index into entries array\n",
    "        epoch: Int; number of epochs traversed\n",
    "        total_entries: Int; number of entries in the database\n",
    "        entries: np.array; parsed entries of the database\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, loc, shuffle=True):\n",
    "        '''Initialize new DBReader instance.\n",
    "        \n",
    "        Args:\n",
    "            loc: String; path to database\n",
    "            shuffle: Bool; shuffle entries (True) or keep original order (False)\n",
    "        '''\n",
    "        \n",
    "        self.begin = 0\n",
    "        self.epoch = 0\n",
    "        self.__shuffle = shuffle\n",
    "        \n",
    "        # Keep size to maximum of 1MB\n",
    "        self._db = lmdb.open(loc, map_size=1024**3, readonly=True, lock=False)\n",
    "        \n",
    "        with self._db.begin() as txn:\n",
    "            self.total_entries = txn.stat()['entries']\n",
    "            self.entries = np.empty(shape=[self.total_entries], dtype=tuple)\n",
    "            \n",
    "            index = 0\n",
    "            cursor = txn.cursor()\n",
    "            for key, value in cursor:\n",
    "                datum = caffe_pb2.Datum()\n",
    "                datum.ParseFromString(value)\n",
    "                self.entries[index] = (Image.open(StringIO(datum.data)), datum.label)\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "        self.__shuffle_entries()\n",
    "                \n",
    "    def __shuffle_entries(self):\n",
    "        if self.__shuffle: np.random.shuffle(self.entries)\n",
    "                \n",
    "    def __update(self, num):\n",
    "        '''Properly update self.begin index and update epoch counter\n",
    "        \n",
    "        Args:\n",
    "            num: Int; the number to update self.begin\n",
    "        '''\n",
    "        \n",
    "        if num >= len(self.entries):\n",
    "            num = 0\n",
    "            self.epoch += 1\n",
    "            self.__shuffle_entries()\n",
    "        \n",
    "        self.begin = num\n",
    "                \n",
    "    def get_batch(self, batch_size):\n",
    "        '''Get batch_size entries.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Int; number of entries to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            data: np.array; array of batch_size entry objects\n",
    "            labels: np.array; array of labels of entry objects\n",
    "        '''\n",
    "        \n",
    "        end = self.begin + batch_size\n",
    "        tmp = self.entries[self.begin:end] if end < len(self.entries) else self.entries[self.begin:]\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        for img, lbl in tmp:\n",
    "            images.append(np.asarray(img))\n",
    "            labels.append(lbl)\n",
    "\n",
    "        self.__update(end)\n",
    "        return np.stack(images), np.array(labels)\n",
    "    \n",
    "    def reset(self):\n",
    "        '''Reset begin index and epoch count and reshuffle entries.'''\n",
    "        \n",
    "        self.begin = 0\n",
    "        self.epoch = 0\n",
    "        self.__shuffle_entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_db_entries(path):\n",
    "    '''Create a DBEntries if database at path exists.\n",
    "\n",
    "    Args:\n",
    "        path: String; the path the to database\n",
    "\n",
    "    Returns:\n",
    "        DBReader instance if path database exists, None otherwise\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        reader = DBEntries(path)\n",
    "    except lmdb.Error:\n",
    "        reader = None\n",
    "\n",
    "    return reader\n",
    "\n",
    "class Database(object):\n",
    "    '''Handle entry generation from lmdb databases created in DIGITS.\n",
    "    \n",
    "    Attributes:\n",
    "        train: np.array; parsed caffe_pb2.Datum entries for training\n",
    "        valid: np.array; parsed caffe_pb2.Datum entries for validation\n",
    "        test: np.array; parsed caffe_pb2.Datum entries for testing\n",
    "        labels: List[String]; ordered labels of files in the databases\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        '''Initialize new Database instance.\n",
    "        \n",
    "        Args:\n",
    "            root_dir: String; path to root directory of training, validation, and (optional) testing databases\n",
    "        '''\n",
    "        \n",
    "        # Parsed database entries\n",
    "        self.train = create_db_entries(os.path.join(root_dir, 'train_db'))\n",
    "        self.valid = create_db_entries(os.path.join(root_dir, 'val_db'))\n",
    "        self.test = create_db_entries(os.path.join(root_dir, 'test_db'))\n",
    "        \n",
    "        # Mean image\n",
    "        self.mean = np.asarray(Image.open(os.path.join(root_dir, 'mean.jpg')))\n",
    "        self.dims = self.mean.shape\n",
    "        \n",
    "        # Get label mappings\n",
    "        with open(os.path.join(root_dir, 'labels.txt'), 'r') as lbl_file:\n",
    "            self.labels = [x[:-1] for x in lbl_file.readlines()]   # Remove newlines\n",
    "            \n",
    "    def reset(self):\n",
    "        '''Reset database entry handlers.'''\n",
    "        \n",
    "        if self.train is not None: self.train.reset()\n",
    "        if self.valid is not None: self.valid.reset()\n",
    "        if self.test is not None: self.test.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DB in 18.766 seconds\n",
      "((61526,), (20485,), None, (256, 256, 3), (256, 256, 3))\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "db = Database(lmdb_dir)\n",
    "end = time.time()\n",
    "print('Create DB in %.3f seconds' % (end - start))\n",
    "print(db.train.entries.shape, db.valid.entries.shape, db.test, db.mean.shape, db.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights(shape, std, name=None):\n",
    "    '''Create a weights variable.\n",
    "    \n",
    "    Args:\n",
    "        shape: A list of ints\n",
    "        std: A float; standard deviation of randomized filler values\n",
    "        name: String; a name for the returned tensor\n",
    "        \n",
    "    Returns:\n",
    "        A variable tensor\n",
    "    '''\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, dtype=tf.float32, stddev=std), name=name)\n",
    "\n",
    "def biases(num, shape, name=None):\n",
    "    '''Create a biases variable.\n",
    "    \n",
    "    Args:\n",
    "        num: The constant value to fill the variable\n",
    "        shape: A list of ints; specify the dimensions of returned tensor\n",
    "        name: String; a name for the returned tensor\n",
    "        \n",
    "    Returns:\n",
    "        A variable tensor\n",
    "    '''\n",
    "    return tf.Variable(tf.constant(num, dtype=tf.float32, shape=shape), name=name)\n",
    "\n",
    "def pool(conv, name=None):\n",
    "    '''Perform max pooling on conv.\n",
    "    \n",
    "    Args:\n",
    "        conv: A 4-D tensor; the convolution on which to perform max pooling\n",
    "        name: String; a name for the returned tensor\n",
    "        \n",
    "    Returns:\n",
    "        Max pooled tensor\n",
    "    '''\n",
    "    return tf.nn.max_pool(conv, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID', name=name)\n",
    "\n",
    "def convolution(data, bias_val, weights_shape, stride, pad, name):\n",
    "    '''Convolution layer of AlexNet.\n",
    "    \n",
    "    Args:\n",
    "        data: A 4-D tensor; input on which to perform the convolution\n",
    "        bias_val: A float; see biases() for more details\n",
    "        weights_shape: A list of ints; 1-D of length 4; see weights() for more details\n",
    "        stride: A list of ints; 1-D of length 4; The stride of the sliding window for each dimension of data\n",
    "        pad: Type of padding to use; either SAME or VALID\n",
    "        name: String; a name for the convolution layer\n",
    "        \n",
    "    Returns:\n",
    "        A tensor; the result of the convolution layer\n",
    "    '''\n",
    "    \n",
    "    with tf.name_scope(name) as scope:\n",
    "        kern = weights(weights_shape, 1e-2, name + '_weights')\n",
    "        conv = tf.nn.conv2d(data, kern, stride, padding=pad)\n",
    "        bias = biases(bias_val, [weights_shape[-1]], name + '_biases')\n",
    "        conv = tf.nn.relu(tf.nn.bias_add(conv, bias), name=scope)\n",
    "        return conv\n",
    "    \n",
    "def fully_connected(data, bias_val, weights_shape, keep_prob, name):\n",
    "    '''Fully Connected layer of AlexNet.\n",
    "    \n",
    "    Args:\n",
    "        data: A 4-D tensor; input on which to perform the matrix multiplication\n",
    "        bias_val: A float; see biases() for more details\n",
    "        weights_shape: A list of ints; 1-D of length 4; see weights() for more details\n",
    "        keep_prob: A 0-D tensor; probability of keeping activation during dropout\n",
    "        name: String; a name for the fully connected layer\n",
    "        \n",
    "    Returns:\n",
    "        A tensor; the result of the fully connected layer\n",
    "    '''\n",
    "    \n",
    "    with tf.name_scope(name) as scope:\n",
    "        weight = weights(weights_shape, 5e-3, name + '_weights')\n",
    "        bias = biases(bias_val, [weights_shape[-1]], name + '_biases')\n",
    "        relu = tf.nn.relu_layer(data, weight, bias)\n",
    "        drop = tf.nn.dropout(relu, keep_prob, name=scope)\n",
    "        return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alexnet(images, keep_prob, out_chan):\n",
    "    '''Create AlexNet Model.\n",
    "    \n",
    "    Based on the graph visualization from DIGITS.\n",
    "    DIGITS runs LRN after convolutions 1 and 2; however, there\n",
    "    is no GPU implementation available for TF, resulting in\n",
    "    massive performance decrease. According to the Stanford \n",
    "    Course [CS321n](http://cs231n.github.io/convolutional-networks/),\n",
    "    LRN has minimal effect on outcome, and is thus removed from this model.\n",
    "    \n",
    "    Args:\n",
    "        images: Images tensor with shape [batch_size, 256, 256, 3]\n",
    "        keep_prob: A 0-D tensor; see fully_connected() for more details\n",
    "        out_chan: Dictionary; key: name of layer, val: number of output channels\n",
    "        \n",
    "    Returns:\n",
    "        logits: Last tensor in fully connected layer\n",
    "    '''\n",
    "    \n",
    "    # conv1\n",
    "    conv1 = convolution(images, 0.0, [11, 11, 3, out_chan['conv1']], [1, 4, 4, 1], 'VALID', 'conv1')\n",
    "    \n",
    "    # norm1\n",
    "    #norm1 = tf.nn.local_response_normalization(conv1, alpha=1e-4, beta=0.75, name='norm1')\n",
    "    \n",
    "    # pool1\n",
    "    pool1 = pool(conv1, 'pool1')\n",
    "    \n",
    "    # conv2\n",
    "    conv2 = convolution(pool1, 0.1, [5, 5, out_chan['conv1'], out_chan['conv2']], [1, 1, 1, 1], 'SAME', 'conv2')\n",
    "    \n",
    "    # norm2\n",
    "    #norm2 = tf.nn.local_response_normalization(conv2, alpha=1e-4, beta=0.75, name='norm2')\n",
    "    \n",
    "    # pool2\n",
    "    pool2 = pool(conv2, 'pool2')\n",
    "    \n",
    "    # conv3\n",
    "    conv3 = convolution(pool2, 0.0, [3, 3, out_chan['conv2'], out_chan['conv3']], [1, 1, 1, 1], 'SAME', 'conv3')\n",
    "    \n",
    "    # conv4\n",
    "    conv4 = convolution(conv3, 0.1, [3, 3, out_chan['conv3'], out_chan['conv4']], [1, 1, 1, 1], 'SAME', 'conv4')\n",
    "    \n",
    "    # conv5\n",
    "    conv5 = convolution(conv4, 0.1, [3, 3, out_chan['conv4'], out_chan['conv5']], [1, 1, 1, 1], 'SAME', 'conv5')\n",
    "    \n",
    "    # pool5\n",
    "    pool5 = pool(conv5, 'pool5')\n",
    "    \n",
    "    # Flatten each image to apply regression\n",
    "    flat = tf.reshape(pool5, [-1, out_chan['flat']])\n",
    "    \n",
    "    # fc6\n",
    "    fc6 = fully_connected(flat, 0.1, [out_chan['flat'], out_chan['fc6']], keep_prob, 'fc6')\n",
    "    \n",
    "    # fc7\n",
    "    fc7 = fully_connected(fc6, 0.1, [out_chan['fc6'], out_chan['fc7']], keep_prob, 'fc7')\n",
    "    \n",
    "    # fc8\n",
    "    # Similar to fully connected layers 6 and 7\n",
    "    # above, but without relu or dropout\n",
    "    with tf.name_scope('fc8') as scope:\n",
    "        weight = weights([out_chan['fc7'], out_chan['fc8']], 1e-2, 'fc8_weights')\n",
    "        bias = biases(0.0, [out_chan['fc8']], 'fc8_biases')\n",
    "        fc8 = tf.nn.bias_add(tf.matmul(fc7, weight), bias, name=scope)\n",
    "        \n",
    "        return fc8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    '''Calculate loss from logits and true labels.\n",
    "    \n",
    "    Args:\n",
    "        logits: Logits tensor, tf.float32 - [batch_size, num_categories]\n",
    "        labels: True labels tensor, tf.int32 - [batch_size]\n",
    "        \n",
    "    Returns:\n",
    "        loss: Loss tensor, tf.float32\n",
    "    '''\n",
    "    \n",
    "    with tf.name_scope('loss') as scope:\n",
    "        \n",
    "        # Convert labels to 1-hot encodings\n",
    "        batch_size = tf.size(labels)\n",
    "        labels = tf.expand_dims(labels, 1)\n",
    "        indices = tf.expand_dims(tf.range(0, batch_size), 1)\n",
    "        concated = tf.concat(1, [indices, labels])\n",
    "        onehot = tf.sparse_to_dense(concated, tf.pack([batch_size, num_categories]), 1.0, 0.0)\n",
    "\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, onehot)\n",
    "        return tf.reduce_mean(cross_entropy)\n",
    "\n",
    "def training(loss, base_lr, decay_steps):\n",
    "    '''Create an optimizer and apply gradients to all trainable variables.\n",
    "    \n",
    "    Args:\n",
    "        loss: Loss tensor, from loss()\n",
    "        base_lr: The base learning rate for gradient descent\n",
    "        decay_steps: An integer; number of global steps before base_lr is dropped by decay rate\n",
    "        \n",
    "    Returns:\n",
    "        train: The training Operation\n",
    "    '''\n",
    "    \n",
    "    with tf.name_scope('training') as scope:\n",
    "        \n",
    "        # Set exponential decay of learning rate\n",
    "        # Use 0.1 decay rate and staircase from DIGITS\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(base_lr, global_step, decay_steps, 0.1, staircase=True)\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        return optimizer.minimize(loss, global_step=global_step), learning_rate\n",
    "\n",
    "def evaluate(logits, labels):\n",
    "    '''Evaluate the accuracy of logits in predicting labels.\n",
    "    \n",
    "    Should only be used with validation images and labels.\n",
    "    \n",
    "    Args:\n",
    "        logits: Logits tensor, tf.float32 - [batch_size, num_categories]\n",
    "        labels: True labels tensor, tf.int32 = [batch_size]\n",
    "        \n",
    "    Returns:\n",
    "        accuracy: The accuracy in percentage of the logits\n",
    "    '''\n",
    "    \n",
    "    with tf.name_scope('evaluate') as scope:\n",
    "        \n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        num_true = tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "        return tf.cast(num_true, tf.float32) * (100. / tf.cast(tf.size(labels), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_logging(log_fname, name):\n",
    "    '''Initialize logging to log_fname.\n",
    "    \n",
    "    Args:\n",
    "        log_fname: Name of log file to save output\n",
    "        name: Name of logger\n",
    "        \n",
    "    Returns:\n",
    "        logger: A logging instance\n",
    "    '''\n",
    "    \n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False\n",
    "    \n",
    "    # Handler to append log file\n",
    "    fh = logging.FileHandler(log_fname)\n",
    "    fh.setLevel(logging.INFO)\n",
    "\n",
    "    # Handler to print logs to console\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "\n",
    "    # Format messages\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "\n",
    "    # Set handlers\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_directory(dir_name):\n",
    "    '''Create a new directory for the run.\n",
    "    \n",
    "    Args:\n",
    "        dir_name: String; the name of the directory for the run\n",
    "        \n",
    "    Returns:\n",
    "        String; new directory path\n",
    "    '''\n",
    "    \n",
    "    new_dir_path = os.path.join(out_dir, dir_name)\n",
    "    \n",
    "    #os.makedirs(new_dir_path, exist_ok=True)   # python 3\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)   # python 2\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    return new_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(run_str, batch_size, epochs, drop_prob, base_lr, chan_dict):\n",
    "    '''Train AlexNet on VDC Dataset.\n",
    "    \n",
    "    Args:\n",
    "        run_str: String; run id\n",
    "        batch_size: Number of examples per batch\n",
    "        epochs: The number of epochs to run the training\n",
    "        drop_prob: Float; [0. - 1.], percentage of logits to keep during dropout\n",
    "        base_lr: The base learning rate for gradient descent\n",
    "        chan_dict: Dictionary; key: name of layer, val: number of output channels\n",
    "        \n",
    "    Returns:\n",
    "        dur_per_batch: List[float]; Times to run each training batch\n",
    "        dur_per_epoch: List[float]; Times to run each training epoch\n",
    "    '''\n",
    "    \n",
    "    run_dir = init_directory(run_str)\n",
    "    logger = init_logging(os.path.join(run_dir, run_str + '.log'), run_str + '_logger')\n",
    "    steps_per_epoch = int(db.train.total_entries / batch_size) + 1\n",
    "    steps_per_print = int(0.2 * steps_per_epoch)\n",
    "    target_size = 227\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        ### Placeholders\n",
    "        \n",
    "        # Placeholder for probability of keeping activations\n",
    "        # during dropout in order to disable dropout with validation\n",
    "        keep_prob = tf.placeholder('float')\n",
    "        \n",
    "        # Placeholders for inputs\n",
    "        x = tf.placeholder(tf.float32, shape=[None, db.dims[0], db.dims[1], db.dims[2]])\n",
    "        l = tf.placeholder(tf.int32, shape=[None])\n",
    "        \n",
    "        ### Process input images\n",
    "\n",
    "        # Resize each image to target_size X target_size X 3\n",
    "        x_in = tf.image.resize_images(x, target_size, target_size, align_corners=True)\n",
    "        \n",
    "        ### Training\n",
    "        \n",
    "        logits = alexnet(x_in, keep_prob, chan_dict)\n",
    "        loss_val = loss(logits, l)\n",
    "        train_op, lr = training(loss_val, base_lr, 10 * steps_per_epoch)\n",
    "        \n",
    "        ### Evaluation\n",
    "        \n",
    "        accuracy = evaluate(logits, l)\n",
    "        \n",
    "        ### Summaries\n",
    "        \n",
    "        # Training\n",
    "        with tf.name_scope('train_sum') as scope:\n",
    "            img_sum = tf.image_summary('images', x_in, collections=[scope])\n",
    "            logits_sum = tf.histogram_summary('logits', logits, collections=[scope])\n",
    "            loss_sum = tf.scalar_summary('loss', loss_val, collections=[scope])\n",
    "            lr_sum = tf.scalar_summary('learning_rate', lr, collections=[scope])\n",
    "            train_sum = tf.merge_all_summaries(key=scope)\n",
    "        \n",
    "        # Evaluation\n",
    "        with tf.name_scope('eval_sum') as scope:\n",
    "            acc_sum = tf.scalar_summary('accuracy', accuracy, collections=[scope])\n",
    "            eval_sum = tf.merge_all_summaries(key=scope)\n",
    "        \n",
    "        ### Initialize session\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        session = tf.Session()\n",
    "        session.run(tf.initialize_all_variables())\n",
    "        sum_writer = tf.train.SummaryWriter(run_dir, session.graph)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "        \n",
    "        step = 0\n",
    "        dur_spp = 0.0\n",
    "        dur_spe = 0.0\n",
    "        \n",
    "        dur_per_batch = []\n",
    "        dur_per_epoch = []\n",
    "\n",
    "        try:\n",
    "            cur_epoch = -1\n",
    "            while db.train.epoch < epochs and not coord.should_stop():\n",
    "                \n",
    "                images, labels = db.train.get_batch(batch_size)\n",
    "                \n",
    "                start = time.time()\n",
    "                feed = {keep_prob: drop_prob, x: images, l: labels}\n",
    "                _, rate, t_loss, t_sum = session.run([train_op, lr, loss_val, train_sum], feed_dict=feed)\n",
    "                dur = time.time() - start\n",
    "                \n",
    "                dur_spp += dur\n",
    "                dur_spe += dur\n",
    "                dur_per_batch.append(dur)\n",
    "                \n",
    "                if step % steps_per_print == 0:\n",
    "                    \n",
    "                    # Log training stats every steps_per_print steps\n",
    "                    logger.info('%s: (%d/%d) :: TRAINING :: loss = %.5f, lr = %.5f, dur = %.3f seconds' %\n",
    "                                (datetime.now(), db.train.epoch, step, t_loss, rate, dur_spp))\n",
    "                    \n",
    "                    dur_spp = 0.0\n",
    "                    \n",
    "                    # Log training summaries every steps_per_print steps\n",
    "                    sum_writer.add_summary(t_sum, step)\n",
    "                \n",
    "                if cur_epoch != db.train.epoch:\n",
    "                    \n",
    "                    cur_epoch = db.train.epoch\n",
    "                    images, labels = db.valid.get_batch(batch_size)\n",
    "                    \n",
    "                    feed = {keep_prob: 1.0, x: images, l: labels}\n",
    "                    acc, v_loss, e_sum = session.run([accuracy, loss_val, eval_sum], feed_dict=feed)\n",
    "                    \n",
    "                    # Log evaluation stats every epoch\n",
    "                    logger.info('%s: (%d/%d) :: VALIDATION :: loss = %.5f, accuracy = %.2f, dur = %.3f seconds' %\n",
    "                                (datetime.now(), db.train.epoch, step, v_loss, acc, dur_spe))\n",
    "                    \n",
    "                    dur_per_epoch.append(dur_spe)\n",
    "                    dur_spe = 0.0\n",
    "                    \n",
    "                    # Log evaluation summaries every epoch\n",
    "                    sum_writer.add_summary(e_sum, step)\n",
    "                    \n",
    "                    # Save model every epoch\n",
    "                    saver.save(session, os.path.join(run_dir, 'model_' + str(db.train.epoch) + '.ckpt'))\n",
    "                    \n",
    "                step += 1\n",
    "                \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            logger.info('Stopping on OutOfRangeError. This should not happen!')\n",
    "            \n",
    "        finally:\n",
    "            logger.info('Total runtime: %.3f seconds' % sum(dur_per_batch))\n",
    "            coord.request_stop()\n",
    "            db.reset()\n",
    "            \n",
    "        coord.join(threads)\n",
    "        session.close()\n",
    "        \n",
    "        return dur_per_batch, dur_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_mean_var_stddev(data):\n",
    "    '''Calculate mean, variance, standard deviation of data.\n",
    "    \n",
    "    Args:\n",
    "        data: List[Float]; statistics to analyze\n",
    "        \n",
    "    Returns:\n",
    "        mean: Float; the average of the data\n",
    "        var: Float; the variance of the data\n",
    "        stddev: Float; the standard deviation of the data\n",
    "    '''\n",
    "    \n",
    "    count = len(data)\n",
    "    mean = sum(data) / count\n",
    "    var = sum([(x - mean)**2 for x in data]) / count\n",
    "    stddev = math.sqrt(var)\n",
    "    \n",
    "    return mean, var, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(num_runs, batch_size, epochs, drop_prob, base_lr, chan_dict):\n",
    "    '''Time how long it takes to train AlexNet model on VDC Dataset with TensorFlow.\n",
    "    \n",
    "    Args:\n",
    "        num_runs: Int; number of runs\n",
    "        batch_size: Number of examples per batch\n",
    "        epochs: The number of epochs to run the training\n",
    "        drop_prob: Float; [0. - 1.], percentage of logits to keep during dropout\n",
    "        base_lr: The base learning rate for gradient descent\n",
    "        chan_dict: Dictionary; key: name of layer, val: number of output channels\n",
    "    '''\n",
    "    \n",
    "    dur_total = []\n",
    "    \n",
    "    logger = init_logging(os.path.join(out_dir, 'benchmarking.log'), 'benchmark_logger')\n",
    "    logger.info('%s: Begin benchmarking AlexNet with TensorFlow' % (datetime.now()))\n",
    "    \n",
    "    try:\n",
    "        for i in range(num_runs):\n",
    "            dur_per_batch, dur_per_epoch = train('run' + str(i),\n",
    "                                                 batch_size,\n",
    "                                                 epochs,\n",
    "                                                 drop_prob,\n",
    "                                                 base_lr,\n",
    "                                                 chan_dict)\n",
    "            dur_total.append(sum(dur_per_batch))\n",
    "\n",
    "            # Stats per run\n",
    "            now = datetime.now()\n",
    "            run_mean, run_var, run_stddev = calc_mean_var_stddev(dur_per_batch)\n",
    "            logger.info('%s: Run %d :: %d steps, %.3f +/- %.3f sec / batch' %\n",
    "                        (now, i, len(dur_per_batch), run_mean, run_stddev))\n",
    "            run_mean, run_var, run_stddev = calc_mean_var_stddev(dur_per_epoch)\n",
    "            logger.info('%s: Run %d :: %d epochs, %.3f +/- %.3f sec / epoch' %\n",
    "                        (now, i, len(dur_per_epoch), run_mean, run_stddev))\n",
    "\n",
    "        # Total stats\n",
    "        mean, var, stddev = calc_mean_var_stddev(dur_total)\n",
    "        logger.info('%s: %d Runs :: %.3f +/- %.3f sec / run' % (datetime.now(), num_runs, mean, stddev))\n",
    "        logger.info('batch_size: %d, epochs: %d, drop_prob: %.3f, base_lr: %.3f' %\n",
    "                    (batch_size, epochs, drop_prob, base_lr))\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error('%s' % (e))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dir = '/home/qtb9744/alexNet-TF-0.9/benchmarking'\n",
    "num_categories = len(db.labels)\n",
    "\n",
    "num_runs = 3\n",
    "batch_size = 100\n",
    "epochs = 30\n",
    "drop_prob = 0.5\n",
    "base_lr = 1e-2\n",
    "channels = {\n",
    "    'conv1': 96,\n",
    "    'conv2': 256,\n",
    "    'conv3': 384,\n",
    "    'conv4': 384,\n",
    "    'conv5': 256,\n",
    "    'flat': 6 * 6 * 256,\n",
    "    'fc6': 4096,\n",
    "    'fc7': 4096,\n",
    "    'fc8': num_categories\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark(num_runs, batch_size, epochs, drop_prob, base_lr, channels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
