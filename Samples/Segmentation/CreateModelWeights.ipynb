{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set CUDA and python paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEGNET_DIR = '/opt/SegNet/'\n",
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = SEGNET_DIR + 'caffe-segnet/python'\n",
    "\n",
    "!sudo unlink /usr/local/cuda\n",
    "!sudo ln -s /usr/local/cuda-6.5 /usr/local/cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from skimage.io import ImageCollection\n",
    "\n",
    "caffe_root = SEGNET_DIR + 'caffe-segnet/'\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "import caffe\n",
    "from caffe.proto import caffe_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model, weights, and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This parameter defines which model and weights to use\n",
    "Type = 'bayesian'  # Options: segnet, segnetB, bayesian, bayesianB, webdemo\n",
    "\n",
    "# Set path to model and weights files\n",
    "MODEL_PATH = SEGNET_DIR + 'Models/'\n",
    "WEIGHT_PATH = SEGNET_DIR + 'Models/Training/Final/'\n",
    "\n",
    "# Set model and weights paths\n",
    "if Type == 'segnet':\n",
    "    TRAIN_PROTOTXT = 'segnet_train.prototxt'\n",
    "    CAFFEMODEL = 'segnet_iter_40000.caffemodel'\n",
    "    model   = MODEL_PATH + TRAIN_PROTOTXT\n",
    "    weights = WEIGHT_PATH + CAFFEMODEL\n",
    "elif Type == 'segnetB':\n",
    "    TRAIN_PROTOTXT = 'segnet_basic_train.prototxt'\n",
    "    CAFFEMODEL = 'segnet_basic_iter_10000.caffemodel'\n",
    "    model   = MODEL_PATH + TRAIN_PROTOTXT\n",
    "    weights = WEIGHT_PATH + CAFFEMODEL\n",
    "elif Type == 'bayesian':\n",
    "    TRAIN_PROTOTXT = 'bayesian_segnet_train.prototxt'\n",
    "    CAFFEMODEL = 'bayesian_segnet_iter_100000.caffemodel'\n",
    "    model   = MODEL_PATH + TRAIN_PROTOTXT\n",
    "    weights = WEIGHT_PATH + CAFFEMODEL\n",
    "elif Type == 'bayesianB':\n",
    "    TRAIN_PROTOTXT = 'bayesian_segnet_basic_train.prototxt'\n",
    "    CAFFEMODEL = 'bayesian_segnet_basic_iter_10000.caffemodel'\n",
    "    model   = MODEL_PATH + TRAIN_PROTOTXT\n",
    "    weights = WEIGHT_PATH + CAFFEMODEL\n",
    "elif Type == 'webdemo':\n",
    "    TRAIN_PROTOTXT = 'segnet_train_webdemo.prototxt'\n",
    "    CAFFEMODEL = 'segnet_webdemo.caffemodel'\n",
    "    model   = MODEL_PATH + TRAIN_PROTOTXT\n",
    "    weights = WEIGHT_PATH + CAFFEMODEL\n",
    "\n",
    "# Set output directory for the inference weights\n",
    "outdir = SEGNET_DIR + 'Models/Inference'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_dataset(net_message):\n",
    "    assert net_message.layer[0].type == \"DenseImageData\"\n",
    "    source = net_message.layer[0].dense_image_data_param.source\n",
    "    with open(source) as f:\n",
    "        data = f.read().split()\n",
    "    ims = ImageCollection(data[::2])\n",
    "    labs = ImageCollection(data[1::2])\n",
    "    assert len(ims) == len(labs) > 0\n",
    "    return ims, labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make network testable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_testable(train_model_path):\n",
    "    # load the train net prototxt as a protobuf message\n",
    "    with open(train_model_path) as f:\n",
    "        train_str = f.read()\n",
    "    train_net = caffe_pb2.NetParameter()\n",
    "    text_format.Merge(train_str, train_net)\n",
    "\n",
    "    # add the mean, var top blobs to all BN layers\n",
    "    for layer in train_net.layer:\n",
    "        if layer.type == \"BN\" and len(layer.top) == 1:\n",
    "            layer.top.append(layer.top[0] + \"-mean\")\n",
    "            layer.top.append(layer.top[0] + \"-var\")\n",
    "\n",
    "    # remove the test data layer if present\n",
    "    if train_net.layer[1].name == \"data\" and train_net.layer[1].include:\n",
    "        train_net.layer.remove(train_net.layer[1])\n",
    "        if train_net.layer[0].include:\n",
    "            # remove the 'include {phase: TRAIN}' layer param\n",
    "            train_net.layer[0].include.remove(train_net.layer[0].include[0])\n",
    "    return train_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_files(testable_net_path, train_weights_path, num_iterations,\n",
    "                   in_h, in_w):\n",
    "    # load the train net prototxt as a protobuf message\n",
    "    with open(testable_net_path) as f:\n",
    "        testable_str = f.read()\n",
    "    testable_msg = caffe_pb2.NetParameter()\n",
    "    text_format.Merge(testable_str, testable_msg)\n",
    "    \n",
    "    bn_layers = [l.name for l in testable_msg.layer if l.type == \"BN\"]\n",
    "    bn_blobs = [l.top[0] for l in testable_msg.layer if l.type == \"BN\"]\n",
    "    bn_means = [l.top[1] for l in testable_msg.layer if l.type == \"BN\"]\n",
    "    bn_vars = [l.top[2] for l in testable_msg.layer if l.type == \"BN\"]\n",
    "\n",
    "    net = caffe.Net(testable_net_path, train_weights_path, caffe.TEST)\n",
    "    \n",
    "    # init our blob stores with the first forward pass\n",
    "    res = net.forward()\n",
    "    bn_avg_mean = {bn_mean: np.squeeze(res[bn_mean]).copy() for bn_mean in bn_means}\n",
    "    bn_avg_var = {bn_var: np.squeeze(res[bn_var]).copy() for bn_var in bn_vars}\n",
    "\n",
    "    # iterate over the rest of the training set\n",
    "    for i in xrange(1, num_iterations):\n",
    "        res = net.forward()\n",
    "        for bn_mean in bn_means:\n",
    "            bn_avg_mean[bn_mean] += np.squeeze(res[bn_mean])\n",
    "        for bn_var in bn_vars:\n",
    "            bn_avg_var[bn_var] += np.squeeze(res[bn_var])\n",
    "        print 'progress: {}/{}'.format(i, num_iterations)\n",
    "\n",
    "    # compute average means and vars\n",
    "    for bn_mean in bn_means:\n",
    "        bn_avg_mean[bn_mean] /= num_iterations\n",
    "    for bn_var in bn_vars:\n",
    "        bn_avg_var[bn_var] /= num_iterations\n",
    "\n",
    "    for bn_blob, bn_var in zip(bn_blobs, bn_vars):\n",
    "        m = np.prod(net.blobs[bn_blob].data.shape) / np.prod(bn_avg_var[bn_var].shape)\n",
    "        bn_avg_var[bn_var] *= (m / (m - 1))\n",
    "\n",
    "    # calculate the new scale and shift blobs for all the BN layers\n",
    "    scale_data = {bn_layer: np.squeeze(net.params[bn_layer][0].data)\n",
    "                  for bn_layer in bn_layers}\n",
    "    shift_data = {bn_layer: np.squeeze(net.params[bn_layer][1].data)\n",
    "                  for bn_layer in bn_layers}\n",
    "\n",
    "    var_eps = 1e-9\n",
    "    new_scale_data = {}\n",
    "    new_shift_data = {}\n",
    "    for bn_layer, bn_mean, bn_var in zip(bn_layers, bn_means, bn_vars):\n",
    "        gamma = scale_data[bn_layer]\n",
    "        beta = shift_data[bn_layer]\n",
    "        Ex = bn_avg_mean[bn_mean]\n",
    "        Varx = bn_avg_var[bn_var]\n",
    "        new_gamma = gamma / np.sqrt(Varx + var_eps)\n",
    "        new_beta = beta - (gamma * Ex / np.sqrt(Varx + var_eps))\n",
    "\n",
    "        new_scale_data[bn_layer] = new_gamma\n",
    "        new_shift_data[bn_layer] = new_beta\n",
    "    print \"New data:\"\n",
    "    print new_scale_data.keys()\n",
    "    print new_shift_data.keys()\n",
    "\n",
    "    # assign computed new scale and shift values to net.params\n",
    "    for bn_layer in bn_layers:\n",
    "        net.params[bn_layer][0].data[...] = new_scale_data[bn_layer].reshape(\n",
    "            net.params[bn_layer][0].data.shape\n",
    "        )\n",
    "        net.params[bn_layer][1].data[...] = new_shift_data[bn_layer].reshape(\n",
    "            net.params[bn_layer][1].data.shape\n",
    "        )\n",
    "        \n",
    "    # build a test net prototxt\n",
    "    test_msg = testable_msg\n",
    "    # replace data layers with 'input' net param\n",
    "    data_layers = [l for l in test_msg.layer if l.type.endswith(\"Data\")]\n",
    "    for data_layer in data_layers:\n",
    "        test_msg.layer.remove(data_layer)\n",
    "    test_msg.input.append(\"data\")\n",
    "    test_msg.input_dim.append(1)\n",
    "    test_msg.input_dim.append(3)\n",
    "    test_msg.input_dim.append(in_h)\n",
    "    test_msg.input_dim.append(in_w)\n",
    "    # Set BN layers to INFERENCE so they use the new stat blobs\n",
    "    # and remove mean, var top blobs.\n",
    "    for l in test_msg.layer:\n",
    "        if l.type == \"BN\":\n",
    "            if len(l.top) > 1:\n",
    "                dead_tops = l.top[1:]\n",
    "                for dl in dead_tops:\n",
    "                    l.top.remove(dl)\n",
    "            l.bn_param.bn_mode = caffe_pb2.BNParameter.INFERENCE\n",
    "    # replace output loss, accuracy layers with a softmax\n",
    "    dead_outputs = [l for l in test_msg.layer if l.type in [\"SoftmaxWithLoss\", \"Accuracy\"]]\n",
    "    out_bottom = dead_outputs[0].bottom[0]\n",
    "    for dead in dead_outputs:\n",
    "        test_msg.layer.remove(dead)\n",
    "    test_msg.layer.add(\n",
    "        name=\"prob\", type=\"Softmax\", bottom=[out_bottom], top=['prob']\n",
    "    )\n",
    "    return net, test_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BN calc net...\n",
      "Calculate BN stats...\n",
      "progress: 1/94\n",
      "progress: 2/94\n",
      "progress: 3/94\n",
      "progress: 4/94\n",
      "progress: 5/94\n",
      "progress: 6/94\n",
      "progress: 7/94\n",
      "progress: 8/94\n",
      "progress: 9/94\n",
      "progress: 10/94\n",
      "progress: 11/94\n",
      "progress: 12/94\n",
      "progress: 13/94\n",
      "progress: 14/94\n",
      "progress: 15/94\n",
      "progress: 16/94\n",
      "progress: 17/94\n",
      "progress: 18/94\n",
      "progress: 19/94\n",
      "progress: 20/94\n",
      "progress: 21/94\n",
      "progress: 22/94\n",
      "progress: 23/94\n",
      "progress: 24/94\n",
      "progress: 25/94\n",
      "progress: 26/94\n",
      "progress: 27/94\n",
      "progress: 28/94\n",
      "progress: 29/94\n",
      "progress: 30/94\n",
      "progress: 31/94\n",
      "progress: 32/94\n",
      "progress: 33/94\n",
      "progress: 34/94\n",
      "progress: 35/94\n",
      "progress: 36/94\n",
      "progress: 37/94\n",
      "progress: 38/94\n",
      "progress: 39/94\n",
      "progress: 40/94\n",
      "progress: 41/94\n",
      "progress: 42/94\n",
      "progress: 43/94\n",
      "progress: 44/94\n",
      "progress: 45/94\n",
      "progress: 46/94\n",
      "progress: 47/94\n",
      "progress: 48/94\n",
      "progress: 49/94\n",
      "progress: 50/94\n",
      "progress: 51/94\n",
      "progress: 52/94\n",
      "progress: 53/94\n",
      "progress: 54/94\n",
      "progress: 55/94\n",
      "progress: 56/94\n",
      "progress: 57/94\n",
      "progress: 58/94\n",
      "progress: 59/94\n",
      "progress: 60/94\n",
      "progress: 61/94\n",
      "progress: 62/94\n",
      "progress: 63/94\n",
      "progress: 64/94\n",
      "progress: 65/94\n",
      "progress: 66/94\n",
      "progress: 67/94\n",
      "progress: 68/94\n",
      "progress: 69/94\n",
      "progress: 70/94\n",
      "progress: 71/94\n",
      "progress: 72/94\n",
      "progress: 73/94\n",
      "progress: 74/94\n",
      "progress: 75/94\n",
      "progress: 76/94\n",
      "progress: 77/94\n",
      "progress: 78/94\n",
      "progress: 79/94\n",
      "progress: 80/94\n",
      "progress: 81/94\n",
      "progress: 82/94\n",
      "progress: 83/94\n",
      "progress: 84/94\n",
      "progress: 85/94\n",
      "progress: 86/94\n",
      "progress: 87/94\n",
      "progress: 88/94\n",
      "progress: 89/94\n",
      "progress: 90/94\n",
      "progress: 91/94\n",
      "progress: 92/94\n",
      "progress: 93/94\n",
      "New data:\n",
      "[u'conv1_1_bn', u'conv4_2_bn', u'conv2_2_D_bn', u'conv5_2_D_bn', u'conv4_3_bn', u'conv5_3_bn', u'conv1_2_bn', u'conv3_3_bn', u'conv4_1_D_bn', u'conv3_3_D_bn', u'conv3_1_D_bn', u'conv3_1_bn', u'conv5_1_D_bn', u'conv3_2_D_bn', u'conv3_2_bn', u'conv5_3_D_bn', u'conv2_2_bn', u'conv2_1_D_bn', u'conv5_1_bn', u'conv4_1_bn', u'conv4_2_D_bn', u'conv4_3_D_bn', u'conv2_1_bn', u'conv1_2_D_bn', u'conv5_2_bn']\n",
      "[u'conv1_1_bn', u'conv4_2_bn', u'conv2_2_D_bn', u'conv5_2_D_bn', u'conv4_3_bn', u'conv5_3_bn', u'conv1_2_bn', u'conv3_3_bn', u'conv4_1_D_bn', u'conv3_3_D_bn', u'conv3_1_D_bn', u'conv3_1_bn', u'conv5_1_D_bn', u'conv3_2_D_bn', u'conv3_2_bn', u'conv5_3_D_bn', u'conv2_2_bn', u'conv2_1_D_bn', u'conv5_1_bn', u'conv4_1_bn', u'conv4_2_D_bn', u'conv4_3_D_bn', u'conv2_1_bn', u'conv1_2_D_bn', u'conv5_2_bn']\n",
      "Saving test net weights...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    caffe.set_mode_gpu()\n",
    "    \n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    print 'Building BN calc net...'\n",
    "    testable_msg = make_testable(model)\n",
    "    BN_calc_path = os.path.join(\n",
    "        outdir, '__for_calculating_BN_stats_' + os.path.basename(model)\n",
    "    )\n",
    "    with open(BN_calc_path, 'w') as f:\n",
    "        f.write(text_format.MessageToString(testable_msg))\n",
    "        \n",
    "    # use testable net to calculate BN layer stats\n",
    "    print 'Calculate BN stats...'\n",
    "    train_ims, train_labs = extract_dataset(testable_msg)\n",
    "    train_size = len(train_ims)\n",
    "    minibatch_size = testable_msg.layer[0].dense_image_data_param.batch_size\n",
    "    num_iterations = train_size // minibatch_size + train_size % minibatch_size\n",
    "    in_h, in_w =(360, 480)\n",
    "    test_net, test_msg = make_test_files(BN_calc_path, weights, num_iterations,\n",
    "                                         in_h, in_w)\n",
    "    \n",
    "    # save deploy prototxt\n",
    "    #print 'Saving deployment prototxt file...'\n",
    "    #test_path = os.path.join(out_dir, 'deploy.prototxt')\n",
    "    #with open(test_path, 'w') as f:\n",
    "    #    f.write(text_format.MessageToString(test_msg))\n",
    "        \n",
    "    print 'Saving test net weights...'\n",
    "    if Type == 'segnet':\n",
    "        test_net.save(os.path.join(outdir, 'test_weights_segnet.caffemodel'))\n",
    "    elif Type == 'segnetB':\n",
    "        test_net.save(os.path.join(outdir, 'test_weights_segnet_basic.caffemodel'))\n",
    "    elif Type == 'bayesian':\n",
    "        test_net.save(os.path.join(outdir, 'test_weights_bayesian_segnet.caffemodel'))\n",
    "    elif Type == 'bayesianB':\n",
    "        test_net.save(os.path.join(outdir, 'test_weights_bayesian_segnet_basic.caffemodel'))\n",
    "    elif Type == 'webdemo':\n",
    "        test_net.save(os.path.join(outdir, 'test_weights_webdemo.caffemodel'))\n",
    "    \n",
    "    print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
